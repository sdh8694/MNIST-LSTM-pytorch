{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SbowtLYYAYt",
        "outputId": "2b29c680-4a43-4d38-d64f-c2b3e0a4e8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "\n",
        "        # One time step\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 28, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        # out.size() --> 100, 10\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "input_dim = 28\n",
        "hidden_dim = 100\n",
        "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
        "output_dim = 10\n",
        "\n",
        "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "\n",
        "# JUST PRINTING MODEL & PARAMETERS \n",
        "print(model)\n",
        "print(len(list(model.parameters())))\n",
        "for i in range(len(list(model.parameters()))):\n",
        "    print(list(model.parameters())[i].size())\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "\n",
        "\n",
        "# Number of steps to unroll\n",
        "seq_dim = 28  \n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        # outputs.size() --> 100, 10\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = (100 * correct).numpy() / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMModel(\n",
            "  (lstm): LSTM(28, 100, num_layers=3, batch_first=True)\n",
            "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "14\n",
            "torch.Size([400, 28])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400])\n",
            "torch.Size([400])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400])\n",
            "torch.Size([400])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400])\n",
            "torch.Size([400])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n",
            "Iteration: 500. Loss: 2.3088889122009277. Accuracy: 11.35\n",
            "Iteration: 1000. Loss: 2.2979414463043213. Accuracy: 11.35\n",
            "Iteration: 1500. Loss: 2.085137367248535. Accuracy: 23.58\n",
            "Iteration: 2000. Loss: 0.820839524269104. Accuracy: 66.82\n",
            "Iteration: 2500. Loss: 0.5265824198722839. Accuracy: 85.89\n",
            "Iteration: 3000. Loss: 0.33715736865997314. Accuracy: 92.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPgfrdUrXdeT"
      },
      "source": [
        "for test_images, test_labels in test_loader:  \n",
        "    sample_image = test_images[3]    # Reshape them according to your needs.\n",
        "    sample_label = test_labels[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCNpHsNRYKH8"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjhOnya0Y1oi",
        "outputId": "73b79835-b19b-45ef-84dc-001de5fa9d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print(sample_label)\n",
        "plt.imshow(sample_image[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2c6e9a93c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}